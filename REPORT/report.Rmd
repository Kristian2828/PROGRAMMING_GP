---
title: "**SONGS CLASSIFICATION FROM SPOTIFY DATASETS YEAR 2020**"
author: "Kristian S.D. (s2043845), Joshah T.Y.Z. (s2138074), Sharmini J.P. (s2114594), Min J.E. (s2104353), Khai M.Q. (s2027890)"
date: "8th June 2022"
output: 
  html_document:
    theme: bootstrap
    css: styles.css
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
options(knitr.table.format = "html")
```
# {.tabset}


## **INTRODUCTION**
`r text_spec("**1. Background**", background = "#D05A6E", color = "white", bold = T)`<br> 

**In the entertainment industry, there is an increase demand for variety of forms and expressions in entertainment to be represented in the community. Music is one form of entertainment highly demanded by all-ages within the market. As the society advances in along with the changes in social science and politics, this leads to a change in the form of music represented or expresses the condition of the community at the point of time. For instance, Rhythm and Blues (R&B) is popular in the 80's while Rap and Reggae is popular in the 90's and Electronic Dance Music (EDM) is formed in early 21st century. Songs classification is a topic which seeks to understand the user's interest to a specific genre based on their previous historical selection of songs. This research seeks to understand what are the variables that would be contribute as an insight and utilise machine learning such as XGBoost to classify the appropriate song genre as a recommendation.**

<br>`r text_spec("**2. Problem Statement**", background = "#D05A6E", color = "white", bold = T)`<br>

 **In this section we have identified two problems and its corresponding question regarding song recommendation based on users' historical song selection.
**1) 
Problem: What would be the preferred song genre based on available independent variables which are components of a song?
Question: How would classification model be able to identify the best recommended genre for the user based on the available independent variables?

**2)
Problem: What independent variables from the historical song record would be able to predict the popularity of the song?
Question: How would regression model be able to predict the popularity of a song which are dependent on the component of the song?


                 key + mode + duration_ms

<br>`r text_spec("**3. Objectives**", background = "#D05A6E", color = "white", bold = T)`<br> 

**We would be performing the procedures as below:**<br>
  1) Performing Data Cleaning and extract datasets for Year 2020.<br>
  2) Performing Data Analysis and identifying relationship between variables.<br>
  3) Implementing Machine Learning models such as classification and regression.<br>

<br>`r text_spec("**4. Details' Dataset**", background = "#D05A6E", color = "white", bold = T)`<br> 

**The dataset is collected from Spotify and stored in Kaggle with the topic "Spotify Top Tracks Popularity Analysis". For this research, we have chosen to only look at dataset from Year 2020 which has 11607 and variables is as follows:**<br>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(DT)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Printing the table
variables = c("country", "popularity", "title", "artist", "album_single",	"artist_followers", "explicit",	"album",	"release_date",	"track_number", "tracks_in_album",	"danceability",	"energy",	"key",	"loudness",	"mode", "speechiness",	"acoustics",	"instrumentalness",	"liveliness",	"valence",	"tempo",	"duration_ms",	"time_signature",	"genre_new",	"days_since_release", "cluster")
explanation = c("Global and 34 countries where Spotify operates",
                "The popularity score calculated taking into account both the number of days a song stayed in the Top200 and the position it stayed in every day, weighting more the top positions",
                "Name of a song",
                "Name of the songs' artist",
                "Whether the song was published as a single or as part of an album or compilation",
                "The number of followers the artist has on Spotify on the 5th of November 2020",
                "Whether the song is rated as ‘Parental Advisory Explicit Content’ or not",
                "Name of the album the song belongs to",
                "Date on which the song was published",
                "The position of the song on its respective album",
                "Total songs present in the album",
                "How suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable",
                "It is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy",
                "The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1",
                "The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db",
                "indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0",
                "Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks",
                "A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic",
                "Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness: value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0",
                "Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live",
                "A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)",
                "The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration",
                "The duration of the track in milliseconds",
                "An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure)",
                "The predominant genre of an artist according to our reclassification of Spotify’s 1200 genres",
                "Number of days passe from the release of the track",
                "Cluster of songs according to genre")
dat = data.frame(variables,explanation)
datatable(dat, rownames=FALSE, filter = 'top', options = list(pageLength = 5), class = 'hover cell - border stripe', colnames = c("VARIABLES","DESCRIPTION"))

#knitr::kable(dat, "simple", col.names = c("**ATTRIBUTES**", "**DESCRIPTION**"), align = c("cc"))
```

<br>


## **DATA PREPARATION** {.tabset}

### **PACKAGES**
<center>**This is a list of the packages we use for this project, along with their functions.**</center><br>

`r text_spec("**Library used to clean the data**", background = "#D05A6E", color = "white", bold = T)`
```{r message=FALSE, warning=FALSE}
library(dplyr) #to data manipulation easier
library(tidyr) #to simplify the process of creating tidy data
library(janitor) #to examine and clean dirty data
library(tidyverse) #an opinionated collection of R packages designed for data science
library(tibble) #To display data along with data type while displaying
library(grid) #To provide a set of functions and classes that represent graphical objects or grobs, that can be manipulated like any other R object
library(ggplot2) #To data visualization
library(knitr) #To enable integration of R code into LaTeX, LyX, HTML, Markdown, AsciiDoc, and reStructuredText documents
library(kableExtra) #To help building common complex tables and manipulate table styles
library(workflows) #A container object that aggregates information required to fit and predict from a model
library(pacman) #Management tool that combines the functionality of base library related functions into intuitively named functions
library(GGally) #A plotting system based on the grammar of graphics
library(tidymodels) #A collection of packages for modeling and machine learning using tidyverse principles
library(tune) #To facilitate hyperparameter tuning for the tidymodels packages
library(kableExtra) #To help you build common complex tables and manipulate table styles
options(knitr.table.format = "html")
library(data.table) #An extension of data. frame package in R
library(DT) #To provide an R interface to the JavaScript library DataTables
library(metan) #A collection of functions that implement a workflow-based approach
library(ranger) #A fast implementation of random forests
library(xgboost) #Extreme Gradient Boosting
library(visdat) #To provide visualisations of an entire dataframe at once
library(magrittr)#To provide a mechanism for chaining commands with a new forward-pipe operator, %>%.
library(hexbin) #A hexbin chart
library(cowplot) #A simple add-on to ggplot
library(reshape2) #to transform data between wide and long formats
library(mgcv) #To create generalized addictive model (GAM)
library(visreg) #For regression visualization
```

### **DESCRIPTION OF DATA**
<center>**The dataset was obtained from Kaggle, and the link source can be found here:** <br>
https://www.kaggle.com/datasets/pepepython/spotify-huge-database-daily-charts-over-3-years</center><br>


`r text_spec("**1. Importing the dataset which is obtained from Github**", background = "#D05A6E", color = "white", bold = T)`
```{r}
url = "https://raw.githubusercontent.com/Kristian2828/PROGRAMMING_GP/main/DATASET/RAW%20DATASET/dataset.csv"
df1 = read.csv(url)
```

`r text_spec("**2. Description of the dataset**", background = "#D05A6E", color = "white", bold = T)`
```{r message=FALSE, warning=FALSE}
#Dimension of the dataset
yes_0 = str_trim(paste0(dim(df1), sep = "  ", collapse = ""))

#Column names of the dataset
vars_0 = str_trim(paste0(names(df1), sep = " | ", collapse = ""))

#Creating a table
table_attributes = data_frame(Data = 'Spotify_Dataset_BeforeCleaning',
  `Rows  Columns` = yes_0,
  Variables = vars_0)
```

<br><center>**The raw dataset's attributes and observations are identified below.**</center><br>
```{r}
#Printing the table
kable(table_attributes, format = "html") %>%
  kable_styling(bootstrap_options = "striped") %>%
     column_spec(2, width = "12em")
```

### **CLEANING AND DATA MANIPULATION**
`r text_spec("**1. Exploration of dataset**", background = "#D05A6E", color = "white", bold = T)`
```{r results='hide'}
# View the structure of the data
glimpse(df1)
```
```{r include = FALSE}
# View a summary of the data
summary(df1)

# View first 6 rows
head(df1)

# View last 6 rows
tail(df1)
```

`r text_spec("**2. Cleaning the data**", background = "#D05A6E", color = "white", bold = T)`
```{r results='hide'}
# Format variable names
df2 = clean_names(df1)
View(df2)

# Since these removing these observations are not expected to affect our approach, we remove these incomplete observations
# Removing NA values
df2 = na.omit(df2)

# Removing irrelevant columns
df2 = df2 %>% select(-c(uri,genre))
df3 = df2[-c(27:148)]
df3
rownames(df3) = seq(length=nrow(df3))
```

`r text_spec("**3. Creating box plot of the audio features to find any extreme outliers**", background = "#D05A6E", color = "white", bold = T)`
```{r}
plot1 = boxplot(df3$loudness,
  ylab = "loudness",
  main = "Boxplot of loudness",
  ylim = c(-30, 0))

plot2 = boxplot(df3$danceability,
  ylab = "dancebility",
  main = "Boxplot of dancebility")
 
plot3 = boxplot(df3$energy,
  ylab = "energy",
  main = "Boxplot of energy")

plot4 = boxplot(df3$acoustics,
  ylab = "acoustics",
  main = "Boxplot of acoustics")

plot5 = boxplot(df3$tempo,
 ylab = "tempo",
 main = "Boxplot of tempo")
```

`r text_spec("**4. Finishing touches**", background = "#D05A6E", color = "white", bold = T)`
```{r}
#dimension of the dataset
yes = str_trim(paste0(dim(df3), sep = "  ", collapse = ""))

#column names of the dataset
vars = str_trim(paste0(names(df3), sep = " | ", collapse = ""))

# Creating a table
table_attributes = data_frame(Data = 'Spotify_Dataset_AfterCleaning',
  `Rows  Columns` = yes,
  Variables = vars)
```

<br><center>**The dataset's attributes and observations upon cleaning and manipulation are identified below.**</center><br>

```{r}
# Printing the table
kable(table_attributes, format = "html") %>%
  kable_styling(bootstrap_options = "striped") %>%
     column_spec(2, width = "12em")
```

<br><center>**Exporting the dataset**</center><br>
```{r}
write.csv(df3, "data_cleaned.csv")
```
<br><br><br>

### **DATA OVERVIEW**
<br><center>**Here is an overview at the dataset after cleaning and manipulation**</center><br>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#datatable(head(df3, 100))
datatable(df3, extensions = 'Buttons', rownames=FALSE, filter = 'top', options = list(pageLength = 10, dom ='Bfrtip', 
                         buttons =c('copy', 'csv','excel','pdf', 'print'), class = 'hover cell - border stripe'))
```

## **DATA ANALYSIS** {.tabset}

### **GENRE**
`r text_spec("**1. Genres Spread**", background = "#D05A6E", color = "white", bold = T)`
```{r}
df3 %>% group_by(genre_new) %>%
  summarise(No_of_tracks = n()) %>% knitr::kable()
```
<br>
```{r warning=FALSE, message=FALSE}
#plot variable  distributions 
vis_dat(df3)
```

`r text_spec("**1. Statistics for each feature (predictor & target)**", background = "#D05A6E", color = "white", bold = T)`
```{r, cache=TRUE}
bees = df3
summary(bees)

#Determine numeric variables
isnum <- names(bees[, sapply(bees, is.numeric)])
isnum
paste("Total numeric variables: ", length(isnum))
```

<h4><center>**Explanation**</center></h4><br>
<center>After cleaning, There are total of 28 column variables, in which, 19 of them are numeric and continuous which can be analysed upon.</center>
<br><br><center><h4>**Enough of Numbers!! Lets see some graphs...**</h4></center>
<center><h4>**Distribution of each numeric variable**</h4></center>

```{r ,figures-side-hist, fig.show="hold",out.width="50%",warning=FALSE, message=FALSE}
hist(bees$key,main="Histogram for Key",xlab="Key",col='blue') 
hist(bees$energy,main="Histogram for Energy",xlab="Energy",col='blue')
hist(bees$danceability,main="Histogram for Danceability",xlab="Danceability",col='blue')
hist(bees$loudness,main="Histogram for loudness",xlab="Loudness",col='blue')
hist(bees$speechiness,main="Histogram for Speechiness",xlab="Speechiness",col='blue')
hist(bees$liveliness,main="Histogram for Liveliness",xlab="Liveliness",col='blue')
hist(bees$instrumentalness,main="Histogram for Instrumentalness",xlab="Instrumentaless",col='blue')
hist(bees$acoustics,main="Histogram for acoustics",xlab="Acoustics",col='blue')
hist(bees$valence,main="Histogram for valence(song mood)",xlab="Valence(song mood)",col='blue')
hist(bees$popularity,main="Histogram for popularity",xlab="Popularity",col='blue')
hist(bees$tempo,main="Histogram for tempo",xlab="Tempo",col='blue')
hist(bees$duration_ms,main="Histogram for song_duration",xlab="Song_Duration",col='blue')
```

<h4><center>**Explanation**</center></h4><br>

<p>**Based on the distribution seen above, there are some notable highlights:**
<ul><li>Most of the variables are showing continuous distribution except for **Key** variable as it is a discrete variable.</li>
<li>For **Intrumentalness** and **Popularity** variable is right-skewed(positive) where mode is on the lower end </li></ul><br>

<h4><center>**Heatmap correlation between features**</center></h4><br>
``` {r, cache=TRUE}
#Set related columns (only numeric)
relevant_col = c("key","energy","danceability","loudness","speechiness","liveliness","instrumentalness","acoustics","valence","popularity", "tempo","duration_ms")
corr_mat <- round(cor(x=bees[relevant_col], y=bees[relevant_col]),2)
melt_corr_mat <- melt(corr_mat)
#Plot heatmap
ggplot(data = melt_corr_mat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()+
  scale_fill_gradient(low = "#DC1C13",
                      high = "#00ff00",
                      guide = "colorbar")+
  geom_text(aes(Var2, Var1, label = value),
            color = "white", size = 3)
```
<h4><center>**Explanation**</center></h4><br>
<br><p>**Based on the correlation heatmap seen above, *GREEN* indicates highest/positive correlation *RED* indicates the lowest/negative relations.**
<ul><li>Highest *positive correlation* is 76% or 0.35 shown between **loudness** and **energy** variable indicates </li>
<li>The second highest *positive correlation* is 35% or 0.35 shown between **valence(song mood)** and **danceability** *indicates* </li>
<li>Another strong relationship can be seen between **acoustics** and **energy** which is 60% or 0.60, though it is **negatively correlated** *indicates* with more acoustics the less energising the music becomes.</li>
<li>Another strong relationship can be seen between **acoustics** and **loudness** which is 56% or 0.56, though it is **negatively correlated** *indicates* higher loudness when there are less acoustics.</li></ul><br>

<br><h4><center>**Visualise correlation through graphs**</center></h4><br>

```{r,figures-side-correlation, fig.show="hold",out.width="50%" ,cache=TRUE, warning=FALSE, message=FALSE}
ggplot(bees, aes(x = energy, y = loudness)) +
  geom_point() +
  stat_smooth(method = "lm",
              col = "#C42126",
              se = FALSE,
              size = 1)
ggplot(bees, aes(x = acoustics, y = energy)) +
  geom_point() +
  stat_smooth(method = "lm",
              col = "#C42126",
              se = FALSE,
              size = 1)
ggplot(bees, aes(x = valence, y = danceability)) +
  geom_point() +
  stat_smooth(method = "lm",
              col = "#C42126",
              se = FALSE,
              size = 1)
ggplot(bees, aes(x = acoustics, y = loudness)) +
  geom_point() +
  stat_smooth(method = "lm",
              col = "#C42126",
              se = FALSE,
              size = 1)
```

<h4><center>**Explanation**</center></h4><br>
<ul><li>Graph shows strong positive correlation between loudness and energy with linear distribution.</li></ul>

<br><h4><center>**Hexplot to show correlation**</center></h4><br>

```{r,figures-side-hexplot, fig.show="hold",out.width="50%"}
hexbinplot(energy~loudness,data=bees,colramp=function(n)rev(rainbow(64)))
hexbinplot(acoustics~energy,data=bees,colramp=function(n)rev(rainbow(64)))
hexbinplot(danceability~valence,data=bees,colramp=function(n)rev(rainbow(64)))
hexbinplot(acoustics~loudness,data=bees,colramp=function(n)rev(rainbow(64)))
```


### **POPULAR SONGS**

```{r,figures-side-barplot, fig.show="hold",out.width="50%"}

#Popularity by Genre (Barplot)
genre_pop <- bees %>% group_by(genre_new) %>% summarise(n_popularity=sum(popularity))
barplot(n_popularity~genre_new,data=genre_pop,main="Top Genre by Popularity",las=2,cex.names=.8,col=c("red","blue","green"))

##No of tracks by Genre
track<- bees %>% group_by(genre_new) %>% summarise(No_of_tracks = n())
barplot(No_of_tracks~genre_new,data=track,main="Top Genre by Number of song tracks",las=2,cex.names=.8,col=c("red","blue","green"))

##Danceability by Genre
dance<- bees %>% group_by(genre_new) %>% summarise(danceability = sum(danceability))
barplot(danceability~genre_new,data=dance, main="Top Genre by Danceability",las=2,cex.names=.8,col=c("red","blue","green"))

#Liveliness by Genre
live<- bees %>% group_by(genre_new) %>% summarise(liveliness = sum(liveliness))
barplot(liveliness~genre_new,data=live,las=2,cex.names=.8,col=c("red","blue","green"))
```
<h4><center>**Explanation**</center></h4><br>
<ul><li>**Pop** genre is the most popular genre mainly due to its *number of tracks released*, *danceability* and *liveliness* </li></ul>
<br><br><br>

## **MACHINE LEARNING** {.tabset}

### **CLASSIFICATION**

```{r, echo=FALSE}
if (!require("pacman")) install.packages("pacman")
```

`r text_spec("**1. Data Balancing Activity**", background = "#D05A6E", color = "white", bold = T)`
```{r results='hide'}
#Sort factors of the type in ascending order
df3[order(df3$genre_new),]

#Reorder type factors by frequency
df3$genre_new <- fct_infreq(df3$genre_new)

#Now reverse order (so it appears as decreasing the plot)
df3$genre_new <- fct_rev(df3$genre_new)
```

`r text_spec("**2. Plot overview**", background = "#D05A6E", color = "white", bold = T)`
```{r}
# music genres counts and percentages
plot = ggplot(data = df3, aes(y=genre_new)) +
  geom_bar(aes(fill=genre_new)) +
  theme (plot.title = element_text(hjust = 0.5)) +
  expand_limits(x = 8500) +
  labs(title = "Music genres - Number of Cases and Percentages",
                 y = "Genres",
                 x = "Count") +
  geom_text(stat='count', 
            aes(label = sprintf('%s (%.1f%%)', 
                after_stat(count), 
                after_stat(count / sum(count) * 100))),
            hjust=ifelse(1.5, -0.1, 1.1))

plot
```

`r text_spec("**3. Running the Shannon diversity index to see how the data balance is**", background = "#D05A6E", color = "white", bold = T)`
```{r}
#balance evaluation by computing the normalized Shannon diversity index
balance = df3%>% 
  group_by(genre_new) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(check = - (n / sum(n) * log(n / sum(n))) / log(length(genre_new)) ) %>% 
  summarise(balance = sum(check)) %>% 
  pull()

balance
```
`r text_spec("**4. Assessing the Feature using Logistic Regression**", background = "#D05A6E", color = "white", bold = T)`
```{r warning=FALSE}
# compute logistic regression with glm function
genremodel <- glm(genre_new ~ acoustics + danceability + energy + instrumentalness + liveliness + speechiness + tempo + valence +
                 key + mode + duration_ms, 
               data = df3, family = binomial)

# see model output
summary(genremodel)
```
```{r warning=FALSE}
# compute logistic regression with glm function
correlation <- corr_coef(df3[,12:26])

# see model output
plot(correlation)
```

`r text_spec("**5. Classification model: Implementation and Validation**", background = "#D05A6E", color = "white", bold = T)`

<br>**a. Feature Selection**<br>
```{r}
#Subset with selected audio feature predictors 
df4 <- subset(df3, select=c("acoustics", "danceability","energy","speechiness","valence","genre_new"))

#Check the subset
names(df4)
```

```{r}
str(df4)
```
```{r}
summary(df4)
```
<br>**b. Data Splitting**<br>
```{r}
#randomize data
set.seed(123)

#split the data into training (80%) and testing (20%)
df4_split = initial_split(df4, prop = 0.80, strata = genre_new)
df4_split # proportions of cases (train/test/total)
```
```{r}
#training and testing sets extraction
df4_train = training(df4_split)
df4_test = testing(df4_split)

#training and testing dataset inspection
str(df4_train)
```
```{r}
str(df4_test)
```
```{r}
dim(df4_train)
```
```{r}
dim(df4_test)
```
<br>**c. Data Preprocessing Recipe**<br>
```{r}
#Recipe for the model creation
model_recipe <- recipe(genre_new ~ ., data = df4_train) %>% 
  step_normalize(all_numeric())

model_recipe
```

<br>**d. Model Specification**<br>
```{r warning=FALSE}

#Random Forest
rf_model = 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

#K-Nearest Neighbor
knn_model = 
  nearest_neighbor(neighbors = 4) %>% # 
  set_engine("kknn") %>% 
  set_mode("classification") 

#Boosted Trees
boost_model = 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 
```


<br>**e. Workflows**<br>
```{r}
#workflow for Random Forest
rf_wf = workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(model_recipe)

#workflow for K-Nearest Neighbor
knn_wf = workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(model_recipe)

#workflow for Boosted  model
boost_wf = workflow() %>% 
  add_model(boost_model) %>% 
  add_recipe(model_recipe)
```


<br>**f. Folding**<br>
```{r }
#split the train set into folds
set.seed(123)
folds <- vfold_cv(data = df4_train, v = 5, strata = "genre_new")
```


<br>**g. Tuning parameters and Model evaluation**<br>
```{r warning=FALSE, message=FALSE}
# tune Random Forest model 
set.seed(123)
rf_tune =
  rf_wf %>% 
  fit_resamples(
    resamples = folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

# report evaluation metrics for the Random Forest model
rf_tune %>%
  collect_metrics()
```

```{r warning=FALSE, message=FALSE}
#tuning the Boosted Trees model
set.seed(123)
boost_tune = boost_wf %>% 
  fit_resamples(
    resamples = folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 
  
# report evaluation metrics for the Boosted Trees model
boost_tune %>%
  collect_metrics()
```

```{r warning=FALSE, message=FALSE}
#tuning K-Nearest Neighbor model 
set.seed(123)
knn_tune = 
  knn_wf %>% 
  fit_resamples(
    resamples = folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 
  
# report evaluation metrics for the K-Nearest Neighbor model 
knn_tune %>%
  collect_metrics()
```


<br>**h. Best performing model comparison**<br>
```{r}
# report highest precision score
best_resamples = 
  bind_rows(
            show_best(rf_tune, metric = "precision", n = 1) %>% mutate(model = "Random Forest") %>% select(model, precision = mean),
            show_best(knn_tune, metric = "precision", n = 1) %>% mutate(model = "K-NN") %>%            select(model, precision = mean), 
            show_best(boost_tune, metric = "precision", n = 1) %>% mutate(model = "Boosted Trees") %>% select(model, precision = mean)
  )

report_1 = best_resamples %>% 
  arrange(desc(precision)) %>% 
  knitr::kable("simple")
report_1
```
```{r}
# report highest recall score
best_resamples = 
  bind_rows(
            show_best(rf_tune, metric = "recall", n = 1) %>% mutate(model = "Random Forest") %>% select(model, recall = mean),
            show_best(knn_tune, metric = "recall", n = 1) %>% mutate(model = "K-NN") %>% select(model, recall = mean), 
            show_best(boost_tune, metric = "recall", n = 1) %>% mutate(model = "Boosted Trees") %>% select(model, recall = mean)
  )

report_2 = best_resamples %>% 
  arrange(desc(recall)) %>% 
  knitr::kable("simple")
report_2
```
```{r}
# report highest roc-auc score
best_resamples = 
  bind_rows(
            show_best(rf_tune, metric = "roc_auc", n = 1) %>% mutate(model = "Random Forest") %>% select(model, roc_auc = mean),
            show_best(knn_tune, metric = "roc_auc", n = 1) %>% mutate(model = "K-NN") %>% select(model, roc_auc = mean), 
            show_best(boost_tune, metric = "roc_auc", n = 1) %>% mutate(model = "Boosted Trees") %>% select(model, roc_auc = mean)
  )

report_3 = best_resamples %>% 
  arrange(desc(roc_auc)) %>% 
  knitr::kable("simple")
report_3
```

```{r}
# report highest specificity score
best_resamples = 
  bind_rows(
            show_best(rf_tune, metric = "spec", n = 1) %>% mutate(model = "Random Forest") %>% select(model, spec = mean),
            show_best(knn_tune, metric = "spec", n = 1) %>% mutate(model = "K-NN") %>% select(model, spec = mean), 
            show_best(boost_tune, metric = "spec", n = 1) %>% mutate(model = "Boosted Trees") %>% select(model, spec = mean)
  )

report_4 = best_resamples %>% 
  arrange(desc(spec)) %>% 
  knitr::kable("simple")
report_4
```


<br>
<br>**i. Predictor Test**<br>
```{r, results='hide'}
#Generate predictions from the test set
test_predictions = rf_tune %>% collect_predictions()
test_predictions
```

<br>**j. Final model fitting**<br>
```{r warning=FALSE, message=FALSE}
#fit the final model
last_fit_rf = last_fit(rf_wf, 
                        split = df4_split,
                        metrics = metric_set(
                          recall, precision, f_meas, 
                          accuracy, kap,
                          roc_auc, sens, spec)
                        )

#compute performance score for the final model
last_fit_rf %>% 
  collect_metrics()
```

```{r}
#Confusion matrix with the test set
last_fit_rf %>%
  collect_predictions() %>% 
  conf_mat(genre_new, .pred_class) %>% 
  autoplot(type = "heatmap")
```


### **REGRESSION**

## Check Correlation of Popularity to other numerical and integer variables

##**Check for Normality**
hist(df3$popularity) ##not normal, failed and cannot proceed with linear regression

#Further test on Normality:
full_linregmodel<-lm(popularity ~ artist_followers + explicit + track_number + tracks_in_album
		+ danceability + energy + key + loudness + mode + speechiness
		+ acoustics + instrumentalness + liveliness + valence + tempo
		+ duration_ms + time_signature + days_since_release, data=df3[sample(nrow(df3),5000,replace=FALSE),])
ols_test_normality(full_linregmodel)
ols_plot_resid_qq(full_linregmodel)
ols_plot_resid_hist(full_linregmodel)


### Linear regression model is not suitable as the variables do not follow the assumption of normality
`r text_spec("**1. Check for Normality of the dependent variable: Popularity**", background = "#D05A6E", color = "white", bold = T)`
```{r results='hide'}
hist(df3$popularity)
##dependent variable not normal, failed and cannot proceed with parametric linear regression analysis. We will consider non-linear regressions and non-parametric tests
```

`r text_spec("**2. Check the correlation with dependent variable: Popularity**", background = "#D05A6E", color = "white", bold = T)`
```{r}
Correlationstats<-cor(df3[,c("popularity","danceability","energy"
,"key","loudness","mode","speechiness","acoustics","instrumentalness","liveliness"
,"valence","tempo","duration_ms","time_signature","days_since_release")]);Correlationstats[2:15,1]
## Note that only "danceability","liveliness" and "valance" show significant correlation of >5%.
```

`r text_spec("**3. Overview of the dataset**", background = "#D05A6E", color = "white", bold = T)`
```{r, echo=FALSE}
df5<-subset(df3, select = c("popularity","danceability","liveliness","valence"))
datatable(df5, extensions = 'Buttons', rownames=FALSE, filter = 'top', options = list(pageLength = 5, dom ='Bfrtip', 
                         buttons =c('copy', 'csv','excel','pdf', 'print'), class = 'hover cell - border stripe'))
```

`r text_spec("**4.0. Using Generalized Addictive Model**", background = "#D05A6E", color = "white", bold = T)`
```{r}
```

`r text_spec("**4.1. Fit a model with multiple regression with Linear Fit**", background = "#D05A6E", color = "white", bold = T)`
```{r}
multiple.regression_lm<- gam(popularity ~ danceability + liveliness + valence, data=df5)
coef(multiple.regression_lm); AIC(multiple.regression_lm) 
visreg(multiple.regression_lm,"danceability")
visreg(multiple.regression_lm,"liveliness")
visreg(multiple.regression_lm,"valence")
multiple.regression_lm$model #The linear regression model data

```

`r text_spec("**4.2. Fit a model with multiple regression with Non-Linear Fit**", background = "#D05A6E", color = "white", bold = T)`
```{r}
multiple.regression_gam <- gam(popularity ~ s(danceability) + s(liveliness) + s(valence), data=df5)
coef(multiple.regression_gam); AIC(multiple.regression_lm) 
visreg(multiple.regression_gam,"danceability")
visreg(multiple.regression_gam,"liveliness")
visreg(multiple.regression_gam,"valence")
multiple.regression_gam$model #The non-linear regression model data
```

`r text_spec("**5. Summary of the model**", background = "#D05A6E", color = "white", bold = T)`
```{r}
summary(multiple.regression_lm)
summary(multiple.regression_gam)
anova(multiple.regression_1, multiple.regression_2, test="Chisq")

##This shows additional statistical evidence to suggest that incorporating nonlinear relationships of the covariates improves the model.
```


```{r}
plot(multiple.regression,col=c("red","blue"))
```
<br><br>


## **ASSESSMENT**

`r text_spec("**1. Classification models analysis**", background = "#D05A6E", color = "white", bold = T)`

**A Shannon diversity index of 0.65 was obtained from the dataset. Based on this value, we can conclude that the genre classes in the data are fairly distributed. Following that, a subset with the selected features was created to start processing the classifier models. The new dataset contains five (5) predictors which are acousticsness, danceability, energy, speechiness, and valence, as well as the genre as the outcome variable.**

**For this analysis, we allocated 80% of the data to the train set and 20% of the remaining data to the test set. The recipe, model specification, and workflow are three objects that were built to tune and evaluate predictive models. Furthermore, the algorithms chosen for this analysis were Random Forest, K-Nearest Neighbor, and Boosted Tree, with a validation set of 5 folds.**

**By classification analysis, the results of the algorithms showed that the Random Forest model outperformed the other algorithms. Precision and recall scores were significantly lower than specificity and roc-auc scores. This implies that the model may be better at detecting true positives and true negatives than false positives and false negatives. Besides that, because specificity was the highest performance score among all evaluation metrics, the model may be particularly well suited to detecting true negatives.**

**By regression analysis, the dependent variable, popularity, shows little to no correlation with the other numerical and integer variable of the dataset. However, by utilising variables such as danceability, livliness, and valence which has correlation of at least 5% would allow to consider non-linear regression application such as the generalized addictive model to fit a nonparametric curve to the data without specifying any particular mathematical model to describe the nonlinearity. The model overall showed smoothed fitting, with the stated three variables showing significant relation to dependent variable, popularity. Further research would need to be look into for a non-parametric fitted curve in determining the popularity of the song based on independent variables.**


## **CONCLUSION**
**In conclusion, the experiment have shown significant insights on song recommendation to users based on the extracted component songs listened collectively by the respective user.
**Insights:
1)The data cleaning portion required more attention which raises concerns on the initial data collection process. 
2)Training the machine learning models with data throughout other years would be required to develop an accurate machine learning models
**We continue to invite further research into data collected from other years, example 2019, 2018 etc. Overall, machine learning models with classification and regression analysis have the capability to futher recommend song preference tailored to a user's historical records.

## **AUTHORS**

<center>**PREPARED BY**</center><br>
```{r echo=FALSE}
student = c("**Joshah Tio Yong Zheng**", "**Kristian Surya Dinata**", "**Khai Mun**", "**Min Jie**", "**Sharmini Joseph Pereira**")
matric = c("S2138074","S2043845","S2027890","S2104353","S2114595")
people = data.frame(student,matric)
knitr::kable(people, "simple", col.names = c("Name", "Matric No."), align = c("cc"))
```

